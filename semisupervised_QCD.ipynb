{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEMISUPERVISED.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GMu7vX5V2u8O",
        "IpwFz_zfR0Sq",
        "McSBEfmhFe-s"
      ],
      "authorship_tag": "ABX9TyNaw0d7a0oGiFdSEW0xYQgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrePalermo/ML-lattice/blob/main/semisupervised_QCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcySQw4nkbNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a23bd18-0bf5-42cb-b9d0-ec498fb0391f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xz_5q8dkhwv"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#target_dir = where the folders of the configurations are stored\n",
        "target_dir = './meas_plMap'\n",
        "\n",
        "#Lattice_size is the spatial dimension of the lattices\n",
        "Lattice_size = 32\n",
        "\n",
        "#path = the various folders containing the configurations. The folders are named \"370_res_b1.95n#\" where # is the time extension\n",
        "path = []\n",
        "for a in os.listdir(target_dir):\n",
        "\tif os.path.isdir(target_dir +'/'+ a):\n",
        "\t\tpath.append(a)\n",
        "\t\n",
        "#number_T = the number of different temperature explored\n",
        "number_T = len(path)\n",
        "\n",
        "#this snippet sorts the path vector by name (i.e. by increasing time extension)\n",
        "def sort_name(a):\n",
        "\treturn np.frombuffer(a.encode(), \"uint8\").sum()\n",
        "\n",
        "path.sort(key=sort_name)\n",
        "\n",
        "#list of time spacing used\n",
        "nt = np.array([])\n",
        "for i in range( number_T ):\n",
        "\tnt = np.append( nt, int(path[i][14:]) )\n",
        " \n",
        "#number_conf = how many configurations for each n_t will we use for the program\n",
        "number_conf = 200"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwNKkXZW25GT"
      },
      "source": [
        "#Load the Polyakov loop configurations in a vector\n",
        "super_lattice = np.ones((number_T*number_conf,Lattice_size**3),dtype = complex)\n",
        "for i in range(number_T):\n",
        "  filesnames = os.listdir(target_dir +'/'+ path[i])\n",
        "  np.random.shuffle(filesnames)\n",
        "  for j in range(number_conf):\n",
        "    super_lattice[i*number_conf+j] = np.fromfile(target_dir +'/'+ path[i]+'/'+nomifiles[j],dtype=complex)\n",
        "\n",
        "#The configurations have been loaded, super_lattice is reshaped in its lattice form. y contains the n_t at which each configuration has been obtained\n",
        "y = np.repeat(nt,number_conf)\n",
        "super_lattice = np.reshape(super_lattice,(number_conf*number_T,Lattice_size,Lattice_size,Lattice_size))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acrKKTmR7sV6"
      },
      "source": [
        "#samples selection and preprocessing\n",
        "train_frac = 0.6\n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!uncomment the pdesidered preprocessing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#no preprocessing\n",
        "train_set, validation_set, y_train, y_val = train_test_split(super_lattice,y, train_size=train_frac)\n",
        "\n",
        "# #fluctuations\n",
        "# no_mean = super_lattice-(np.ones(shape=super_lattice.shape).T*super_lattice.mean(axis=(1,2,3))).T\n",
        "# std_dev = np.sqrt(np.mean(np.abs(super_lattice-(np.ones(shape=super_lattice.shape).T*super_lattice.mean(axis=(1,2,3))).T)**2,axis=(1,2,3)))\n",
        "# super_fluctuations = (no_mean.T*std_dev).T\n",
        "# train_set, validation_set, y_train, y_val = train_test_split(super_fluctuations,y, train_size=0.6)\n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "train_conf = int(train_frac*number_conf)\n",
        "#labeled configurations for the semisupervised problem\n",
        "N = np.int(0.3*train_conf) #N=number of labeled configurations\n",
        "labeled_conf = np.ones((2*N,Lattice_size,Lattice_size,Lattice_size),dtype=complex)\n",
        "\n",
        "for i in range(N):\n",
        "  labeled_conf[i] = train_set[y_train==np.min(y_train)][i]\n",
        "  labeled_conf[-i-1] = train_set[y_train==np.max(y_train)][-i-1]\n",
        "\n",
        "#unphysical labels in the latent space\n",
        "conv_labels = np.repeat([0,1],N) \n",
        "conv_labels = np.reshape(conv_labels,(2*N,1))\n",
        "\n",
        "#building channels for the convolutional neural network\n",
        "Re_train = np.real(train_set)\n",
        "Im_train = np.imag(train_set)\n",
        "Re_Im_lattice_train = np.stack( (Re_train,Im_train), axis=4)\n",
        "\n",
        "Re_val = np.real(validation_set)\n",
        "Im_val = np.imag(validation_set)\n",
        "Re_Im_lattice_val = np.stack( (Re_val,Im_val), axis=4)\n",
        "\n",
        "Re_labeled = np.real(labeled_conf)\n",
        "Im_labeled = np.imag(labeled_conf)\n",
        "Re_Im_labeled = np.stack( (Re_labeled,Im_labeled), axis=4)\n",
        "\n",
        "#Normalizing the data by multiplying for a constant \n",
        "norm = 1/np.amax([np.amax(np.abs(Re_Im_lattice_val)),np.amax(np.abs(Re_Im_lattice_train))])\n",
        "\n",
        "Re_Im_lattice_train *= norm\n",
        "Re_Im_lattice_val *= norm \n",
        "Re_Im_labeled *= norm"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNIo91S08O7w",
        "outputId": "edd0d3eb-9bc2-4ae2-c707-267e802ab2b8"
      },
      "source": [
        "#Neural network \n",
        "\n",
        "# conv_encoder = tf.keras.Sequential()\n",
        "# conv_encoder.add( tf.keras.layers.Conv3D(64,(3,3,3),input_shape=(Lattice_size,Lattice_size,Lattice_size,2), activation='swish') )\n",
        "# #conv_encoder.add( tf.keras.layers.Conv3D(64,(3,3,3), activation='swish') )\n",
        "# conv_encoder.add(tf.keras.layers.MaxPooling3D((2,2,2)) )\n",
        "# conv_encoder.add( tf.keras.layers.Conv3D(64,(3,3,3), activation='swish') )\n",
        "# conv_encoder.add( tf.keras.layers.Dropout(0.5) )\n",
        "# #conv_encoder.add( tf.keras.layers.Conv3D(48,(3,3,3), activation='swish') )\n",
        "# conv_encoder.add(tf.keras.layers.MaxPooling3D((4,4,4)) )\n",
        "# conv_encoder.add( tf.keras.layers.Flatten() )\n",
        "# #conv_encoder.add( tf.keras.layers.Dense(128,activation='relu') )\n",
        "# conv_encoder.add( tf.keras.layers.Dense(6,activation='relu') )\n",
        "# conv_encoder.add( tf.keras.layers.Dense(1,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform) ) #linear \n",
        "\n",
        "# conv_decoder = tf.keras.Sequential()\n",
        "# conv_decoder.add( tf.keras.layers.Dense(27, activation='relu',input_shape=np.shape(conv_encoder(Re_Im_lattice_train[0:1]))[1:]) ) #relu\n",
        "# #conv_decoder.add( tf.keras.layers.Dense(128, activation='relu' ))\n",
        "# conv_decoder.add( tf.keras.layers.Dense(27*4, activation='relu' ))\n",
        "# conv_decoder.add( tf.keras.layers.Reshape((3,3,3,4)) )\n",
        "# conv_decoder.add(tf.keras.layers.Conv3DTranspose(64,(3,3,3),activation='relu'))\n",
        "# conv_decoder.add(tf.keras.layers.UpSampling3D((2,2,2)))\n",
        "# conv_decoder.add(tf.keras.layers.Conv3DTranspose(64,(3,3,3),activation='relu')) #relu\n",
        "# conv_decoder.add(tf.keras.layers.Conv3DTranspose(48,(4,4,4),activation='relu')) #relu\n",
        "# #conv_decoder.add(tf.keras.layers.Conv3DTranspose(48,(3,3,3),activation='relu',padding='same'))\n",
        "# conv_decoder.add(tf.keras.layers.UpSampling3D((2,2,2)))\n",
        "# conv_decoder.add(tf.keras.layers.Conv3DTranspose(2,(3,3,3),activation='linear')) #linear\n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "conv_encoder = tf.keras.Sequential()\n",
        "conv_encoder.add( tf.keras.layers.Conv3D(24,(3,3,3),input_shape=(Lattice_size,Lattice_size,Lattice_size,2), activation='swish') )\n",
        "#conv_encoder.add( tf.keras.layers.Conv3D(14,(3,3,3), activation='swish') )\n",
        "conv_encoder.add(tf.keras.layers.MaxPooling3D((2,2,2)) )\n",
        "conv_encoder.add( tf.keras.layers.Conv3D(24,(3,3,3), activation='swish') )\n",
        "conv_encoder.add( tf.keras.layers.Dropout(0.5) )\n",
        "#conv_encoder.add( tf.keras.layers.Conv3D(48,(3,3,3), activation='swish') )\n",
        "conv_encoder.add(tf.keras.layers.MaxPooling3D((4,4,4)) )\n",
        "conv_encoder.add( tf.keras.layers.Flatten() )\n",
        "#conv_encoder.add( tf.keras.layers.Dense(128,activation='relu') )\n",
        "# conv_encoder.add( tf.keras.layers.Dense(2,activation='relu') )\n",
        "conv_encoder.add( tf.keras.layers.Dense(1,activation='swish'))#,kernel_initializer=tf.keras.initializers.RandomUniform) ) #linear \n",
        "\n",
        "conv_decoder = tf.keras.Sequential()\n",
        "conv_decoder.add( tf.keras.layers.Dense(27*4, activation='swish',input_shape=np.shape(conv_encoder(Re_Im_lattice_train[0:1]))[1:]) ) #relu\n",
        "#conv_decoder.add( tf.keras.layers.Dense(128, activation='relu' ))\n",
        "# conv_decoder.add( tf.keras.layers.Dense(27*4, activation='relu' ))\n",
        "conv_decoder.add( tf.keras.layers.Reshape((3,3,3,4)) )\n",
        "conv_decoder.add(tf.keras.layers.Conv3DTranspose(14,(3,3,3),activation='swish'))\n",
        "conv_decoder.add(tf.keras.layers.UpSampling3D((2,2,2)))\n",
        "conv_decoder.add(tf.keras.layers.Conv3DTranspose(14,(3,3,3),activation='relu')) #relu\n",
        "conv_decoder.add(tf.keras.layers.Conv3DTranspose(14,(4,4,4),activation='relu')) #relu\n",
        "#conv_decoder.add(tf.keras.layers.Conv3DTranspose(48,(3,3,3),activation='relu',padding='same'))\n",
        "conv_decoder.add(tf.keras.layers.UpSampling3D((2,2,2)))\n",
        "conv_decoder.add(tf.keras.layers.Conv3DTranspose(2,(3,3,3),activation='tanh')) #linear\n",
        "\n",
        "\n",
        "@tf.function \n",
        "def corr (X_labeled, y_labels):\n",
        "  #correction for the labeled configurations in the latent space\n",
        "  metric = tf.keras.losses.MeanAbsoluteError(reduction='sum_over_batch_size')\n",
        "  return metric(conv_encoder(X_labeled),y_labels)\n",
        "\n",
        "@tf.function \n",
        "def loss_all (X_all):\n",
        "  #loss of the autoencoder: how good the input is reproduced by the NN\n",
        "  metric = tf.keras.losses.MeanAbsoluteError(reduction='sum_over_batch_size')\n",
        "  encoded_all = conv_encoder(X_all)\n",
        "\n",
        "  return metric(conv_decoder(encoded_all),X_all) \n",
        "\n",
        "# @tf.function \n",
        "# def conv_loss (X_labeled, y_labels, X_all):\n",
        "#   metric = tf.keras.losses.MeanAbsoluteError(reduction='sum_over_batch_size')\n",
        "#   encoded_all = conv_encoder(X_all)\n",
        "#   return metric(conv_decoder(encoded_all),X_all) + corr(X_labeled,y_labels) \n",
        "  \n",
        "@tf.function \n",
        "def conv_val_loss (X_all,batch_number,batch_size):\n",
        "  #loss of the autoencoder: same as loss_all but the input data are split in batches before passing them through the nn. This saves memory.\n",
        "  loss = 0\n",
        "  X_random = tf.random.shuffle(X_all)\n",
        "  for j in range(batch_number):\n",
        "    loss += loss_all(X_random[j*batch_size:(j+1)*batch_size])\n",
        "  \n",
        "   \n",
        "  return  loss/batch_number\n",
        "\n",
        "\n",
        "optimizer = tf.optimizers.Adam(0.0005)\n",
        "\n",
        "\n",
        "# @tf.function\n",
        "# def conv_train(X_labeled, y_labeled, X_all):\n",
        "#   with tf.GradientTape() as gt: \n",
        "#     gt.watch ( conv_encoder.variables + conv_decoder.variables )\n",
        "#     loss_ = conv_loss(X_labeled,y_labeled,X_all)\n",
        "#     grads = gt.gradient (loss_, conv_encoder.variables + conv_decoder.variables)\n",
        "#     optimizer.apply_gradients (zip(grads, conv_encoder.variables + conv_decoder.variables))\n",
        "#     return loss_\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_batch(labeled,label,X_all,batch_number,batch_size):\n",
        "  #training function with input split in batches. Returns the total loss\n",
        "  with tf.GradientTape() as gt: \n",
        "    # metric = tf.keras.losses.MeanAbsoluteError(reduction='sum_over_batch_size')\n",
        "    gt.watch ( conv_encoder.variables + conv_decoder.variables )\n",
        "\n",
        "    corr_ = corr(labeled,label) \n",
        "    loss_=0\n",
        "    \n",
        "    X_random = tf.random.shuffle(X_all)\n",
        "    for j in range(batch_number):\n",
        "      loss_ += loss_all(X_random[j*batch_size:(j+1)*batch_size])\n",
        "\n",
        "    loss_= loss_/batch_number + 0.1*corr_\n",
        "    grads = gt.gradient (loss_, conv_encoder.variables + conv_decoder.variables)\n",
        "    optimizer.apply_gradients (zip(grads, conv_encoder.variables + conv_decoder.variables))\n",
        "    return loss_\n",
        "\n",
        "conv_encoder.summary()\n",
        "conv_decoder.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_6 (Conv3D)            (None, 30, 30, 30, 24)    1320      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3 (None, 15, 15, 15, 24)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 13, 13, 13, 24)    15576     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 13, 13, 13, 24)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3 (None, 3, 3, 3, 24)       0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 648)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 649       \n",
            "=================================================================\n",
            "Total params: 17,545\n",
            "Trainable params: 17,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 108)               216       \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 3, 3, 3, 4)        0         \n",
            "_________________________________________________________________\n",
            "conv3d_transpose_12 (Conv3DT (None, 5, 5, 5, 14)       1526      \n",
            "_________________________________________________________________\n",
            "up_sampling3d_6 (UpSampling3 (None, 10, 10, 10, 14)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_transpose_13 (Conv3DT (None, 12, 12, 12, 14)    5306      \n",
            "_________________________________________________________________\n",
            "conv3d_transpose_14 (Conv3DT (None, 15, 15, 15, 14)    12558     \n",
            "_________________________________________________________________\n",
            "up_sampling3d_7 (UpSampling3 (None, 30, 30, 30, 14)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_transpose_15 (Conv3DT (None, 32, 32, 32, 2)     758       \n",
            "=================================================================\n",
            "Total params: 20,364\n",
            "Trainable params: 20,364\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm3tLuS3NFEn",
        "outputId": "03112fc6-8fad-4706-b45e-308c12a35cb5"
      },
      "source": [
        "print((number_conf-train_conf)*number_T/12, train_conf*number_T/12)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60.0 90.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDDx2doV_toa",
        "outputId": "c2307663-ac33-424f-ab87-84a04f8b0151"
      },
      "source": [
        "batch_size_train = 12 #should divide train_conf*number_T\n",
        "batch_number_train = int(train_conf*number_T/batch_size_train)\n",
        "batch_size_val = 12 #should divide (number_conf-train_conf)*number_T\n",
        "batch_number_val = int((number_conf-train_conf)*number_T/batch_size_val)\n",
        "print(\"loss_val = \",conv_val_loss(tf.constant(Re_Im_lattice_val, dtype=tf.float32),batch_number_val,batch_size_val).numpy(),\"\\n\",\n",
        "      \"loss_train = \",conv_val_loss(tf.constant(Re_Im_lattice_train, dtype=tf.float32),batch_number_train,batch_size_train).numpy(),\"\\n\",\n",
        "      \"correction = \",corr(tf.constant(Re_Im_labeled, dtype=tf.float32),tf.constant(conv_labels, dtype=tf.float32)).numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss_val =  0.19224377 \n",
            " loss_train =  0.19272119 \n",
            " correction =  0.504996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUeq6G98-TsW",
        "outputId": "d0216ae3-234e-497c-d80c-761967b16cf5"
      },
      "source": [
        "loss_tot = []\n",
        "autoloss_validation=[]\n",
        "autoloss_train = []\n",
        "\n",
        "X = tf.constant(Re_Im_lattice_train, dtype=tf.float32)\n",
        "X_val = tf.constant(Re_Im_lattice_val, dtype=tf.float32)\n",
        "X_labeled = tf.constant(Re_Im_labeled, dtype=tf.float32)\n",
        "y_label = tf.constant(conv_labels, dtype=tf.float32)\n",
        "\n",
        "\n",
        "batch_size_train = 12 #should divide train_conf*number_T\n",
        "batch_number_train = int(train_conf*number_T/batch_size_train)\n",
        "batch_size_val = 12 #should divide (number_conf-train_conf)*number_T\n",
        "batch_number_val = int((number_conf-train_conf)*number_T/batch_size_val)\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "for i in range(epoch):\n",
        "  #training and saving the various losses\n",
        "  loss_tot.append ( train_batch(X_labeled,y_label,X,batch_number_train,batch_size_train) )\n",
        "  autoloss_validation.append( conv_val_loss(X_val,batch_number_val,batch_size_val) )\n",
        "  autoloss_train.append(conv_val_loss(X,batch_number_train,batch_size_train))\n",
        "  #the following snippet prints the progress of the training \n",
        "  row = \"[\"+\"=\"*np.int(100/epoch *(i+1)) + \">\"+\".\"*(np.int(100-100/epoch *(i+1)))+\"]\"\n",
        "  sys.stdout.write(\"\\r %d%% %s \" %( 100/epoch *(i + 1),row))\n",
        "  sys.stdout.flush()\n",
        "  time.sleep(0.1)\n",
        "\n",
        "sys.stdout.write(\"\\n\")\n",
        "\n",
        "#plot the total loss alone\n",
        "plt.plot ( loss_tot, 'g' ) \n",
        "# plt.savefig('losstot_C_semisup.pdf')\n",
        "\n",
        "#plot the total loss along with training and validation losses for the autoencoder\n",
        "plt.subplots()\n",
        "plt.plot ( loss_tot, 'g' )\n",
        "plt.plot(autoloss_validation,'r')\n",
        "plt.plot(autoloss_train,'b')\n",
        "# plt.savefig('loss_C_semisup.pdf')\n",
        "\n",
        "#plot the training and validation losses for the autoencoder\n",
        "plt.subplots()\n",
        "plt.plot(autoloss_validation,'r')\n",
        "plt.plot(autoloss_train,'b')\n",
        "print(\"Total loss=\",loss_tot[-1].numpy(),'autoencoder loss: train=',autoloss_train[-1].numpy(),' autoencoder loss: validation=',autoloss_validation[-1].numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 95% [===============================================================================================>.....] "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTXzk3_H-feD"
      },
      "source": [
        "def encoder_evaluate(train,validation,batch_size_t,batch_number_t,batch_size_v,batch_number_v):\n",
        "  #find the output of the encoder. This is done in batches to prevent running out of memory.\n",
        "  out_T = []\n",
        "  out_V = []\n",
        "\n",
        "  for j in range(batch_number_t):\n",
        "    out_T.append( conv_encoder(train[j*batch_size_t:(j+1)*batch_size_t]) )\n",
        "\n",
        "  for j in range(batch_number_v):\n",
        "    out_V.append( conv_encoder(val[j*batch_size_v:(j+1)*batch_size_v]) )\n",
        "\n",
        "  return out_T, out_V\n",
        "\n",
        "conv_encoded_train, conv_encoded_val = dajenpo(Re_Im_lattice_train, Re_Im_lattice_val, batch_size_train, batch_number_train,batch_size_val, batch_number_val)\n",
        "\n",
        "\n",
        "plt.plot(y_train,conv_encoded_train,'ro')\n",
        "plt.xlabel('nt')\n",
        "plt.ylabel(r'Encoded',size=15)\n",
        "# plt.savefig('semisupervised_T_fluct.pdf')\n",
        "\n",
        "\n",
        "plt.subplots()\n",
        "plt.plot(y_val,conv_encoded_val,'go')\n",
        "plt.xlabel('nt')\n",
        "plt.ylabel(r'Encoded',size=15)\n",
        "# plt.savefig('semisupervised_V_fluct.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8vulHgScseU"
      },
      "source": [
        "a_qcd = 0.0823*1/200 #MeV\n",
        "T_c = 200 #MeV\n",
        "y_t = conv_encoded_train\n",
        "x_t = 1/T_c *1/(a_qcd*y_train)\n",
        "y_v = conv_encoded_val\n",
        "x_v = 1/T_c *1/(a_qcd*y_val)\n",
        "\n",
        "#plot the figures\n",
        "\n",
        "plt.figure(facecolor=\"#a3c1ad\")\n",
        "plt.title(r\"Training sample\",size=25)\n",
        "# plt.plot(x_train,y_train,'ro')\n",
        "plt.plot(x_t[x_t<=1],y_t[x_t<=1],'ro',label='Phase 1')\n",
        "plt.plot(x_t[x_t>1],y_t[x_t>1],'bo',label='Phase 2')\n",
        "plt.xlabel('$T/T_c$',size=20)\n",
        "plt.ylabel(r'Encoded classifier',size=20)\n",
        "plt.legend()\n",
        "# plt.savefig('QCD_train.pdf',facecolor=\"#a3c1ad\")\n",
        "plt.figure(facecolor=\"#a3c1ad\")\n",
        "plt.title(r\"Validation sample\",size=25)\n",
        "plt.plot(x_v[x_v<=1],y_v[x_v<=1],'go',label='Phase 1')\n",
        "plt.plot(x_v[x_v>1],y_v[x_v>1],'yo',label='Phase 2')\n",
        "# plt.plot(x_val,y_val,'go')\n",
        "plt.xlabel('$T/T_c$',size=20)\n",
        "plt.ylabel(r'Encoded classifier',size=20)\n",
        "plt.legend()\n",
        "# plt.savefig('QCD_val.pdf',facecolor=\"#a3c1ad\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}